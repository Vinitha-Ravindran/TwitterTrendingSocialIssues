{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words=[]\n",
    "lemmatized_token_d2=[]\n",
    "stop_words_removed=[]\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweet_list)):\n",
    "    #tokenize each review in the collection\n",
    "    tokenized_words.append(nltk.word_tokenize(tweet_list[i].lower()))\n",
    "   \n",
    "    #removing all the stop-words and the punctuations\n",
    "    stop_words_removed.append([token for token in tokenized_words[i] if not token in stopwords.words('english') if token.isalpha()])\n",
    "\n",
    "    #lemmatize all the tokens created in the previous step\n",
    "    lemmatized_token_d2.append([lemmatizer.lemmatize(token) for token in stop_words_removed[i] if token.isalpha()])\n",
    "\n",
    "#each of the tokens, after stop words removal, is converted to string\n",
    "lemmatized_string=[]\n",
    "for i in lemmatized_token_d2:\n",
    "    lemmatized_string.append(\" \".join(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines_list = tokenize.sent_tokenize(paragraph)\n",
    "# sentences.extend(lines_list)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer #gives a score to all the sentence\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "a=[]\n",
    "for sentence in lemmatized_string:\n",
    "    print(sentence)\n",
    "    #ss is a dictionary\n",
    "    ss = sid.polarity_scores(sentence) #calculate score for sentence\n",
    "    a.append(ss)\n",
    "#     for k in sorted(ss): #k is the key\n",
    "        \n",
    "#         print('{0}: {1}, '.format(k, ss[k]), end='') #print score along with results\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list = []\n",
    "for i in range(len(a)):\n",
    "    data = a[i].get(\"compound\", \"\")\n",
    "    sen_list.append(data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(tweet_list,columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(sen_list,columns=['sen_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dataset['news'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [i.replace('-filter:retweets','') for i in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"news\"] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([outputdf['news'].reset_index(drop=True),df1.reset_index(drop=True),df2.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.set_index(\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('final_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
